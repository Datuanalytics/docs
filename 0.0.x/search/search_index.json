{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"Datu AI Analyst Build intelligent, data-driven AI agents in just a few lines of code. Get Started Data-Driven Agents <p>Connect to databases, files, APIs, and more. Let your agents analyze, transform, and act on data.</p> Provider Agnostic <p>Use any LLM provider\u2014OpenAI, AWS Bedrock, Anthropic, and more. Switch providers without code changes.</p> Simple Multi-Agent Workflows <p>Compose agents, tools, and workflows with simple primitives for collaboration and orchestration.</p> Production Ready <p>Deploy to containers, serverless, or cloud. Built-in observability, security, and extensibility.</p> Ready to get started? Check out the quickstart guide or book a demo with our team. Quickstart Guide Book a Demo"},{"location":"architecture/","title":"Overview","text":""},{"location":"architecture/#work-in-progress","title":"Work In Progress.","text":""},{"location":"user-guide/","title":"Datu AI Analyst","text":"<p>Datu Core is a Data AI analyst server which is simple to use with your data sources.</p> <p>First, install the Datu Core server:</p> <pre><code>pip install \"datu-core[postgres,sqldb]\"\n</code></pre> <p>Then follow next steps.</p>"},{"location":"user-guide/#features","title":"Features","text":"<ul> <li>Lightweight talk to data: A simple application that can talk with your data.</li> <li>Production ready: Deploy anywhere whether it is in on-premises or cloud.</li> <li>Safety and security as a priority: Protecting the data while interacting with llm.</li> </ul>"},{"location":"user-guide/#next-steps","title":"Next Steps","text":"<p>Ready to learn more? Check out these resources:</p> <ul> <li>Quickstart - A more detailed introduction to Datu core</li> <li>Datasources - Connecting multiple datasources.</li> </ul> <p>Learn how to contribute or join our community discussions to shape the future of Datu \u2764\ufe0f.</p>"},{"location":"user-guide/#warning","title":"Warning!","text":"<p>Datu is in pilot and preview. During this preview period we welcome your feedback. API's may change as we refine the SDK based on user experiences.</p>"},{"location":"user-guide/configurations/","title":"Configurations","text":""},{"location":"user-guide/configurations/#configurations-to-set","title":"Configurations to set","text":"<p>All the configurations that can be used listed below with their description, default values and possible values. set them using environment variables.</p>"},{"location":"user-guide/configurations/#application-run-configurations","title":"Application run configurations","text":""},{"location":"user-guide/configurations/#host","title":"Host","text":"<p>Host where your application will be running.</p> <pre><code>DATU_HOST=0.0.0.0\n</code></pre>"},{"location":"user-guide/configurations/#port","title":"Port","text":"<p>Port where your application listens to.</p> <pre><code>DATU_PORT=8000\n</code></pre>"},{"location":"user-guide/configurations/#openai-key","title":"Openai key","text":"<p>OpenAI llm key.</p> <pre><code>DATU_OPENAI_API_KEY=\n</code></pre>"},{"location":"user-guide/configurations/#schema-configurations","title":"Schema configurations","text":""},{"location":"user-guide/configurations/#schema-refresh-threshold-days","title":"Schema refresh threshold days","text":"<p>Refresh schema in x days.</p> <pre><code>DATU_SCHEMA_REFRESH_THRESHOLD_DAYS=2\n</code></pre>"},{"location":"user-guide/configurations/#schema-cache-file","title":"Schema cache file","text":"<p>Store schema cache to.</p> <pre><code>DATU_SCHEMA_CACHEC_FILE=schema_cache.json\n</code></pre>"},{"location":"user-guide/configurations/#schema-categorical-detection","title":"Schema categorical detection","text":"<p>Detect categorical columns.</p> <pre><code>DATU_SCHEMA_CATEGORICAL_DETECTION=true\n</code></pre>"},{"location":"user-guide/configurations/#schema-sample-limit","title":"Schema sample limit","text":"<p>Schema sample limit to determine categorical values.</p> <pre><code>DATU_SAMPLE_LIMIT=1000\n</code></pre>"},{"location":"user-guide/configurations/#schema-categorical-threshold","title":"Schema categorical threshold","text":"<p>Threshold for column values to be considered as categorical.</p> <pre><code>DATU_SCHEMA_CATEGORICAL_THRESHOLD=10\n</code></pre>"},{"location":"user-guide/configurations/#schemarag-engine","title":"SchemaRAG Engine","text":"<p>Extract relevant part of schema based on user query. Set to True to activate.</p> <pre><code>DATU_ENABLE_SCHEMA_RAG=False\n</code></pre>"},{"location":"user-guide/configurations/#datasource-related-configuration","title":"Datasource related configuration","text":""},{"location":"user-guide/configurations/#dbt-profiles","title":"DBT profiles","text":"<p>Datasource profiles.yml.</p> <pre><code>DATU_DBT_PROFILES=\n</code></pre>"},{"location":"user-guide/configurations/#mcp-related-configuration","title":"MCP related configuration","text":""},{"location":"user-guide/configurations/#enable-mcp-connectivity","title":"Enable MCP Connectivity","text":"<p>Set the following environment variable to enable MCP connectivity in Datu:</p> <pre><code>DATU_ENABLE_MCP=true\n</code></pre>"},{"location":"user-guide/configurations/#mcp-servers-configuration","title":"MCP Servers Configuration","text":"<p>Define your MCP servers in a JSON config file (e.g. mcp_config.json):</p> <pre><code>{\n  \"mcpServers\": {\n    \"sql_generator\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"datu.mcp.tools.sql_generator\"],\n      \"env\": { \"PYTHONPATH\": \".\" }\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/configurations/#telemetry-related-configuration","title":"Telemetry related configuration","text":""},{"location":"user-guide/configurations/#product-telemetry","title":"Product telemetry","text":"<p>Product telemetry is enabled by default and you can disable it by</p> <pre><code>DATU_ENABLE_ANONYMIZED_TELEMETRY=false\n</code></pre>"},{"location":"user-guide/quickstart/","title":"Datu AI Analyst","text":"<p>This quickstart guide shows you how to spin up your Datu server, add datasources and integrate mcp servers to your agent and emit debug logs. After completing this guide you can deploy to production and running at scale.</p>"},{"location":"user-guide/quickstart/#install-datu-from-pypi","title":"Install Datu from PyPI.","text":"<p>First, ensure that you have Python 3.11+ installed.</p> <p>We'll create a virtual environment to install the Datu and its dependencies in to.</p> <pre><code>python -m venv .venv\n</code></pre> <p>And activate the virtual environment:</p> <ul> <li>macOS / Linux: <code>source .venv/bin/activate</code></li> <li>Windows (CMD): <code>.venv\\Scripts\\activate.bat</code></li> <li>Windows (PowerShell): <code>.venv\\Scripts\\Activate.ps1</code></li> </ul> <p>Next we'll install the <code>datu-core</code> server package:</p> <pre><code>pip install \"datu-core[postgres]\"\n</code></pre>"},{"location":"user-guide/quickstart/#how-to-add-datasources","title":"How to add datasources","text":"<p>As per the current design, the application will fetch all the schema that is listed in the profiles.yml. Place profile.yml in the current directory or home directory for the application to fetch or set DATU_DBT_PROFILES=\"path/profiles.yml\". Structure of profiles.yml</p> <pre><code>my_sources:\n  target: dev-postgres # Target is used to select the datasource that is currently active. Change this if you would like to use a different datasource.\n  outputs:\n    dev-postgres:\n      type: postgres\n\n      host: \"{{ env_var('DB_HOST', 'localhost') }}\"  # if a environment variable is supplied that gets priority. This is useful for not hardcoding.\n\n      port: 5432\n      user: postgres\n      password: postgres\n      dbname: my_sap_bronze\n      schema: bronze\n    dev-sqlserver:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # Mandatory for sqlserver.\n      host: localhost\n      port: 1433\n      user: sa\n      password: Password123!\n      dbname: my_sap_bronze\n      schema: bronze\n</code></pre>"},{"location":"user-guide/quickstart/#how-to-run-datu","title":"How to run datu?","text":"<p>After creating the datasources profiles.yml.</p>"},{"location":"user-guide/quickstart/#environment-variables","title":"\ud83d\udd27 Environment Variables","text":"<p>set the following environment variables:  </p> <ul> <li><code>DATU_OPENAI_API_KEY</code> \u2013 your OpenAI API key  </li> <li><code>DATU_DBT_PROFILES</code> \u2013 path to your <code>profiles.yml</code> </li> </ul> <p>Then run</p> <pre><code>datu\n</code></pre>"},{"location":"user-guide/quickstart/#debug-logs","title":"Debug logs","text":"<p>To enable debug logs in Datu server .</p> <p>Environment variables: Set <code>DATU_LOGGING_LEVEL=\"DEBUG\"</code></p>"},{"location":"user-guide/quickstart/#next-steps","title":"Next Steps","text":"<p>Ready to learn more? Check out these resources:</p> <ul> <li>Datasources - Connecting multiple datasources.</li> <li>More configurations - Datu server configurations includes port, schema configurations etc.</li> </ul>"},{"location":"user-guide/MCP/mcp_connectivity/","title":"MCP Connectivity Overview","text":"<p>This directory provides examples and configuration files to help you connect MCP servers with Datu. Datu uses the mcp-use package to implement MCP clients and manage connectivity.</p>"},{"location":"user-guide/MCP/mcp_connectivity/#purpose","title":"Purpose","text":"<p>With Datu, you can connect to your MCP servers and use their tools through the Datu AI Analyst.</p>"},{"location":"user-guide/MCP/mcp_connectivity/#enabling-mcp-servers","title":"Enabling MCP Servers","text":"<p>To enable MCP connectivity, set the following environment variable (see also configurations.md):</p> <pre><code>export DATU_ENABLE_MCP=true\n</code></pre> <p>Alternatively, add it to your .env file:</p> <pre><code>DATU_ENABLE_MCP=true\n</code></pre>"},{"location":"user-guide/MCP/mcp_connectivity/#connecting-mcp-servers","title":"Connecting MCP Servers","text":"<p>MCP servers are defined in a JSON configuration file (e.g. mcp_config.json). Each server entry specifies the command, arguments, and environment needed to start it.</p> <p>Structure of mcp_config.json</p> <pre><code>{\n  \"mcpServers\": {\n    \"sql_generator\": {\n      \"command\": \"python\",\n      \"args\": [\"-m\", \"datu.mcp.tools.sql_generator\"],\n      \"env\": { \"PYTHONPATH\": \".\" }\n    }\n  }\n}\n</code></pre>"},{"location":"user-guide/datasources/datasources/","title":"Datasources Overview","text":"<p>The directory provides a collection of configurations to help you get started with connecting data sources with Datu.</p>"},{"location":"user-guide/datasources/datasources/#purpose","title":"Purpose","text":"<p>With Datu, you can quickly connect to your data sources and turn raw information into actionable insights.</p>"},{"location":"user-guide/datasources/datasources/#how-to-add-datasources","title":"How to add datasources","text":"<p>As per the current design the application will fetch all the schema that is listed in the profiles.yml. It is to avoid fetching the schema every single time.But it will only work on the target datasource that is selected.</p> <p>Structure of profiles.yml</p> <pre><code>datu_demo:\n  target: dev-postgres # Target is used to select the datasource that is currently active. Change this if you would like to use a different datasource.\n  outputs:\n    dev-postgres:\n      type: postgres\n\n      host: \"{{ env_var('DB_HOST', 'localhost') }}\"  # if a environment variable is supplied that gets priority. This is useful for not hardcoding.\n\n      port: 5432\n      user: postgres\n      password: postgres\n      dbname: my_sap_bronze\n      schema: bronze\n    dev-sqlserver:\n      type: sqlserver\n      driver: 'ODBC Driver 18 for SQL Server' # Mandatory for sqlserver.\n      host: localhost\n      port: 1433\n      user: sa\n      password: Password123!\n      dbname: my_sap_bronze\n      schema: bronze\n</code></pre>"},{"location":"user-guide/datasources/datasources/#about-profilesyml","title":"About profiles.yml","text":"<p>Datu core needs profiles.yml file that contains all the datasources configured. If you have used dbt,this is somewhat like to their profiles.yml though not exaclty the same.</p> <pre><code>&lt;profile-name&gt;:\n  target: &lt;target-name&gt; # this is the default target\n  outputs:\n    &lt;target-name&gt;:\n      type: &lt;postgres | sqlserver | other&gt;\n      schema: &lt;schema_identifier&gt;\n\n      ### Look for each datasources specific variables\n      ...\n\n      ...\n\n&lt;profile-name&gt;: # additional profiles\n  ...\n</code></pre>"},{"location":"user-guide/datasources/datasources/#env_var","title":"env_var","text":"<p>You can use <code>env_var</code> with any attribute in the <code>profiles.yml</code> <code>outputs</code> section to load configuration values from environment variables.  </p>"},{"location":"user-guide/datasources/postgres/","title":"Postgres","text":""},{"location":"user-guide/datasources/postgres/#postgres-as-a-datasource","title":"Postgres as a datasource","text":"<p>Install datucore with extras postgres</p> <pre><code>pip install \"datu-core[postgres]\"\n</code></pre> <p>In profiles.yml</p> <pre><code>dev-postgres:\n    type: postgres\n    host: [hostname]\n    user: [username]\n    password: [password]\n    port: [port]\n    dbname: [database name]\n    schema: [schema]\n</code></pre>"},{"location":"user-guide/datasources/sqlserver/","title":"Sqlserver","text":""},{"location":"user-guide/datasources/sqlserver/#sqlserver-as-a-datasource","title":"Sqlserver as a datasource","text":"<p>Install datucore with extras postgres</p> <pre><code>pip install \"datu-core[sqldb]\"\n</code></pre> <p>For sqlserver to work you have to make sure the below ODBC driver is installed on your machine according to the Operating System.</p> <p>Install ODBC driver</p> <p>In profiles.yml</p> <pre><code>dev-sqlserver:\n    type: sqlserver\n    driver: 'ODBC Driver 18 for SQL Server' # Mandatory for sqlserver.\n    host: [hostname]\n    user: [username]\n    password: [password]\n    port: [port]\n    dbname: [database name]\n    schema: [schema]\n</code></pre>"},{"location":"user-guide/deploy/deploy_as_container_service/","title":"Deploying Datu AI Analyst as container service","text":"<p>Use below recommended method to run Datu application as container service.</p>"},{"location":"user-guide/deploy/deploy_as_container_service/#containerization","title":"Containerization","text":"<p>To deploy your Datu, you need to containerize it using Podman or Docker. The Dockerfile defines how your application is packaged and run. Below is an example Docker file that installs all needed dependencies, the application, and configures the FastAPI server to run via unicorn dockerfile.</p> <pre><code>FROM python:3.11-slim\nSHELL [\"/bin/bash\", \"-c\"]\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends build-essential \\\n    curl \\\n    apt-utils \\\n    gnupg2 &amp;&amp;\\\n    rm -rf /var/lib/apt/lists/* &amp;&amp; \\\n    pip install --upgrade pip\n\nRUN curl https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; /etc/apt/trusted.gpg.d/microsoft.gpg \\\n&amp;&amp; curl https://packages.microsoft.com/config/debian/11/prod.list -o /etc/apt/sources.list.d/mssql-release.list\n\nRUN apt-get update\nRUN env ACCEPT_EULA=Y apt-get install -y msodbcsql18\nWORKDIR /app\nCOPY . .\nRUN pip install \"datu-core[postgres,sqldb]\"\nCMD [\"datu\"]\n</code></pre>"},{"location":"user-guide/deploy/deploy_as_container_service/#infrastructure","title":"Infrastructure","text":"<p>For running Datu application smoothly, make sure you are allocated about 1GB of CPU and 2GB of memory atleast.For configurable variables checkout the quickstart/configurations section.</p>"},{"location":"user-guide/telemetry/product/","title":"Telemetry in Datu Server","text":"<p>Datu Server comes with a built-in telemetry system that gathers anonymous usage information. This data helps the maintainers understand how the software is being used and guides ongoing improvements.</p>"},{"location":"user-guide/telemetry/product/#why-telemetry","title":"Why Telemetry?","text":"<p>Telemetry provides valuable feedback without exposing personal or sensitive information. It allows the team to:</p> <ul> <li>Measure how features are being adopted.</li> <li>Detect common usage patterns and performance bottlenecks.</li> <li>Prioritize enhancements that have the greatest impact.</li> <li>Ensure the system evolves in line with real-world needs.</li> </ul>"},{"location":"user-guide/telemetry/product/#what-data-is-collected","title":"What Data Is Collected?","text":"<p>The telemetry system only collects non-identifiable, aggregated data such as:</p> <ul> <li>Basic environment details (e.g., version, operating system).</li> <li>Which features or APIs are being exercised.</li> <li>MCP server names.</li> <li>General performance metrics.</li> </ul> <p>No private data, user content, or identifiers are captured such as: usernames, hostnames, file names, environment variables, or hostnames of systems being tested.</p>"},{"location":"user-guide/telemetry/product/#where-is-telemetry-information-stored","title":"Where is telemetry information stored?","text":"<p>We use Posthog to store and visualize telemetry data.</p> PostHog is an open source platform for product analytics.     Learn more at posthog.com or github.com/posthog."},{"location":"user-guide/telemetry/product/#turning-telemetry-off","title":"Turning Telemetry Off","text":"<p>If you prefer not to share anonymous usage data, you can disable telemetry with an environment variable:</p> <pre><code>export DATU_ENABLE_ANONYMIZED_TELEMETRY=false\n</code></pre>"}]}